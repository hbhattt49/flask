#!/bin/bash
# ===== Spark Setup for Amazon Linux =====

# 1. Update and install Java
sudo yum update -y
sudo yum install -y java-17-openjdk

# 2. Define JAVA_HOME dynamically
export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
echo "export JAVA_HOME=${JAVA_HOME}" | sudo tee -a /etc/profile

# 3. Extract Spark from /tmp
sudo mkdir -p /opt
sudo tar -xvzf /tmp/spark-3-75849c8ad0.5.7-bin-hadoop3.tgz -C /opt
sudo mv /opt/spark-3-75849c8ad0.5.7-bin-hadoop3 /opt/spark
sudo chown -R ec2-user:ec2-user /opt/spark

# 4. Add Spark to PATH
echo "export SPARK_HOME=/opt/spark" | sudo tee -a /etc/profile
echo 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin' | sudo tee -a /etc/profile
source /etc/profile

# 5. Basic Spark config
cat <<EOF | sudo tee /opt/spark/conf/spark-env.sh
export JAVA_HOME=${JAVA_HOME}
export SPARK_MASTER_HOST=\$(hostname -I | awk '{print \$1}')
export SPARK_LOCAL_IP=\$(hostname -I | awk '{print \$1}')
export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8080
export SPARK_WORKER_CORES=2
export SPARK_WORKER_MEMORY=4g
EOF

# 6. Start Spark master (you can start workers later)
sudo -u ec2-user $SPARK_HOME/sbin/start-master.sh
